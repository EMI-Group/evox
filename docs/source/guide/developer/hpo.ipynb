{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Efficient HPO with Evox\n",
    "\n",
    " In this chapter, we will discuss how to use Evox for hyperparameter optimization (HPO).\n",
    " \n",
    " HPO is an essential step in many machine learning tasks, yet it often goes underappreciated. This is mainly due to its heavy computational demands, which can sometimes require days of processing, as well as the difficulties involved in deployment.\n",
    " \n",
    " In Evox, we can easily deploy HPO using the `HPOProblemWrapper`, and achieve efficient computation by leveraging the `vmap` functionality and GPU."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transforming Workflow into Problem\n",
    "\n",
    "```{image} /_static/HPO_structure.svg\n",
    ":alt: HPO structure\n",
    ":width: 700px\n",
    ":align: center\n",
    "```\n",
    "\n",
    "The key to deploy HPO with Evox is to transform the `workflow` into a `problem` with the `HPOProblemWrapper`. After that, we can treat the `workflow` as a common `problem` which has no difference with other problems. The input of the 'HPO problem' is the hyperparameters and the output is the metric.\n",
    "\n",
    "To enable `HPOProblemWrapper` to recognize the hyperparameters, we need to wrap the hyperparameters with `Parameter`. After this simple operation, the hyperparameters will be automatically recognized.\n",
    "```python\n",
    "class ExampleAlgorithm():\n",
    "    def __init__(self,): \n",
    "        self.omega = Parameter([1.0, 2.0]) # wrap the hyperparameters with `Parameter`\n",
    "        self.beta = Parameter(0.1)\n",
    "        pass\n",
    "\n",
    "    def Step(self, key) -> State:\n",
    "        pass\n",
    "```\n",
    "\n",
    "The `HPOProblemWrapper` takes 4 arguments:\n",
    "1. iterations: The number of iterations to be executed in the optimization process.\n",
    "2. num_instances: The number of instances to be executed in parallel in the optimization process.\n",
    "3. workflow: The workflow to be used in the optimization process. Must be wrapped by `core.jit_class`.\n",
    "4. copy_init_state: Whether to copy the initial state of the workflow for each evaluation. Defaults to `True`. If your workflow contains operations that IN-PLACE modify the tensor(s) in initial state, this should be set to `True`. Otherwise, you can set it to `False` to save memory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making Algorithms Parallelizable\n",
    "\n",
    "In order to make the 'inner algorithm' parallelizable, we may need to make some modifications to the algorithm. We have to make sure that the algorithm satisfies the following conditions:\n",
    "1. The algorithm should have no function with in-place operations on the attributes of the algotirhm itself.\n",
    "```python\n",
    "class ExampleAlgorithm():\n",
    "    def __init__(self,): \n",
    "        self.pop = torch.rand(10,10) #attribute of the algotirhm itself\n",
    "        pass\n",
    "\n",
    "    def StepInPlace(self, key) -> State: # function with in-place operations\n",
    "        self.pop.copy_(pop)\n",
    "        pass\n",
    "\n",
    "    def StepNotInPlace(self, state, args) -> State: # function without in-place operations\n",
    "        self.pop = pop\n",
    "        pass\n",
    "```\n",
    "\n",
    "2. The code logic does not rely on conditional control structures.\n",
    "```python\n",
    "class ExampleAlgorithm():\n",
    "    def __init__(self,): \n",
    "        self.pop = rand(10,10) #attribute of the algotirhm itself\n",
    "        pass\n",
    "\n",
    "    def Plus(self, y):\n",
    "        self.pop += y\n",
    "        pass\n",
    "\n",
    "    def Minus(self, y):\n",
    "        self.pop -= y\n",
    "        pass      \n",
    "\n",
    "    def StepWithConditionalControl(self, y) -> State: # function with conditional control\n",
    "        x = rand()\n",
    "        if x>0.5:\n",
    "            self.Plus(y)\n",
    "        else:\n",
    "            self.Minus(y)\n",
    "        pass\n",
    "\n",
    "    def StepWithoutConditionalControl(self, y) -> State: # function without conditional control\n",
    "        x = rand()\n",
    "        cond = x > 0.5\n",
    "        _if_else_ = TracingCond(self.Plus, self.Minus)\n",
    "        _if_else_.cond(cond,y)\n",
    "        self.pop = pop\n",
    "        pass\n",
    "```\n",
    "\n",
    "In Evox, we can easily make the algorithm parallelizable by the `@trace_impl` decorator. \n",
    "\n",
    "The parameter of this decorator is a non-parallelizable function, and the decorated function is a rewrite of the original function. After rewriting, it must support vmap. \n",
    "\n",
    "Under this mechanism, we can retain the original function for use outside HPO tasks while enabling efficient computation within HPO tasks. Moreover, this modification is highly convenient.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utilizing the HPOMonitor\n",
    "\n",
    "We should use `HPOMonitor` in the HPO task to monitor the metric of the algorithm. The `HPOMonitor` only add one method `tell_fitness` comparing with the common monitor. This is designed to make the evaluation metrics for a set of hyperparameters more flexible, as metrics in HPO tasks are often multi-dimensional and complex. \n",
    "\n",
    "Users only need to create a subclass of HPOMonitor and override the tell_fitness method to define their own evaluation metrics.\n",
    "\n",
    "We also provide a simple HPOFitnessMonitor, which supports calculating 'IGD' and 'HV' metrics for both single-objective and multi-objective problems and always uses their minimum value as the evaluation metric for the hyperparameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A simple example\n",
    "\n",
    "Here we would show you a simple example of how to use HPO with Evox. We will use the PSO algorithm to search for the best parameters of a simple algorithm to solve the sphere problem.\n",
    "\n",
    "First, we need to import the necessary modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from evox.algorithms.pso_variants.pso import PSO\n",
    "from evox.core import Algorithm, Mutable, Parameter, Problem, jit_class, trace_impl\n",
    "from evox.problems.hpo_wrapper import HPOFitnessMonitor, HPOProblemWrapper\n",
    "from evox.utils import TracingCond\n",
    "from evox.workflows import EvalMonitor, StdWorkflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we define an simple problem, which is the sphere problem. Note that this has no difference from the `problem`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit_class\n",
    "class Sphere(Problem):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def evaluate(self, x: torch.Tensor):\n",
    "        return (x * x).sum(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can define the algorithm. The oringinal `step` function is non-parallelizable. We rewrite it as `trace_step` to be parallelizable. We modify the in-place opeartions and conditional control."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit_class\n",
    "class ExampleAlgorithm(Algorithm):\n",
    "    def __init__(self, pop_size: int, lb: torch.Tensor, ub: torch.Tensor):\n",
    "        super().__init__()\n",
    "        assert lb.ndim == 1 and ub.ndim == 1, f\"Lower and upper bounds shall have ndim of 1, got {lb.ndim} and {ub.ndim}\"\n",
    "        assert lb.shape == ub.shape, f\"Lower and upper bounds shall have same shape, got {lb.ndim} and {ub.ndim}\"\n",
    "        self.pop_size = pop_size\n",
    "        self.hp = Parameter([1.0, 2.0, 3.0, 4.0]) # the hyperparameters to be optimized\n",
    "        self.lb = lb\n",
    "        self.ub = ub\n",
    "        self.dim = lb.shape[0]\n",
    "        self.pop = Mutable(torch.empty(self.pop_size, lb.shape[0], dtype=lb.dtype, device=lb.device))\n",
    "        self.fit = Mutable(torch.empty(self.pop_size, dtype=lb.dtype, device=lb.device))\n",
    "\n",
    "    def strategy_1(self,pop): # one update strategy\n",
    "        pop = pop * (self.hp[0]+self.hp[1])\n",
    "\n",
    "    def strategy_2(self,pop): #  the other update strategy\n",
    "        pop = pop * (self.hp[2]+self.hp[3])\n",
    "\n",
    "    def step(self):\n",
    "        pop = torch.rand(self.pop_size, self.dim, dtype=self.lb.dtype, device=self.lb.device) # simply random sampling\n",
    "        pop = pop * (self.ub - self.lb)[None, :] + self.lb[None, :]\n",
    "        control_number = torch.rand()\n",
    "        if control_number < 0.5: # conditional control\n",
    "            pop = self.strategy_1(pop)\n",
    "        else:\n",
    "            pop = self.strategy_2(pop)\n",
    "        self.pop.copy_(pop) # in-place update\n",
    "        self.fit.copy_(self.evaluate(pop))\n",
    "\n",
    "    @trace_impl(step) #rewrite the step function to support vmap\n",
    "    def trace_step(self):\n",
    "        pop = torch.rand(self.pop_size, self.dim, dtype=self.lb.dtype, device=self.lb.device)\n",
    "        pop = pop * (self.ub - self.lb)[None, :] + self.lb[None, :]\n",
    "        pop = pop * self.hp[0]\n",
    "        control_number = torch.rand()\n",
    "        cond = control_number < 0.5\n",
    "        _if_else_ = TracingCond(self.strategy_1, self.strategy_2)\n",
    "        _if_else_.cond(cond, pop)\n",
    "        self.pop = pop\n",
    "        self.fit = self.evaluate(pop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we can use the `workflow` to wrap the problem, algorithm and monitor. Then we use the `HPOProblemWrapper` to transform the workflow to a HPO problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_default_device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "inner_algo = ExampleAlgorithm(10, -10 * torch.ones(8), 10 * torch.ones(8))\n",
    "inner_prob = Sphere()\n",
    "inner_monitor = HPOFitnessMonitor()\n",
    "inner_monitor.setup()\n",
    "inner_workflow = StdWorkflow()\n",
    "inner_workflow.setup(inner_algo, inner_prob, monitor=inner_monitor)\n",
    "# Transform the inner workflow to a HPO problem\n",
    "hpo_prob = HPOProblemWrapper(iterations=9, num_instances=7, workflow=inner_workflow, copy_init_state=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can test whether the `HPOProblemWrapper` recognizes the hyperparameters we define. Since we make no modification to the hyperparameters for the 7 instances, the hyperparameters should be the same for all instances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "init params:\n",
      " {'self.algorithm.hp': Parameter containing:\n",
      "tensor([[1., 2., 3., 4.],\n",
      "        [1., 2., 3., 4.],\n",
      "        [1., 2., 3., 4.],\n",
      "        [1., 2., 3., 4.],\n",
      "        [1., 2., 3., 4.],\n",
      "        [1., 2., 3., 4.],\n",
      "        [1., 2., 3., 4.]], device='cuda:0')}\n"
     ]
    }
   ],
   "source": [
    "params = hpo_prob.get_init_params()\n",
    "print('init params:\\n',params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also specify a set of hyperparameter values ourselves. Note that the number of hyperparameter sets must be consistent with the number of instances in the `HPOProblemWrapper`. Note that the custom hyperparameters must be passed in the form of a dictionary and wrapped using the Parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "params:\n",
      " {'self.algorithm.hp': Parameter containing:\n",
      "tensor([[0.6151, 0.4238, 0.3006, 0.0424],\n",
      "        [0.4175, 0.8058, 0.8549, 0.5446],\n",
      "        [0.2170, 0.1184, 0.5396, 0.5979],\n",
      "        [0.5893, 0.6488, 0.3352, 0.4294],\n",
      "        [0.0289, 0.7860, 0.1178, 0.5868],\n",
      "        [0.8950, 0.0965, 0.4080, 0.5824],\n",
      "        [0.9049, 0.5265, 0.1892, 0.5064]], device='cuda:0')} \n",
      "\n",
      "result:\n",
      " tensor([32.5368, 13.1591,  2.6436, 24.7213,  0.0959, 38.7169, 77.4396],\n",
      "       device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "params = hpo_prob.get_init_params()\n",
    "# since we have 7 instances, we need to pass 7 sets of hyperparameters\n",
    "params[\"self.algorithm.hp\"] = torch.nn.Parameter(torch.rand(7, 4), requires_grad=False)\n",
    "result = hpo_prob.evaluate(params)\n",
    "print('params:\\n',params,'\\n')\n",
    "print('result:\\n',result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we use the PSO algorithm to optimize the hyperparameters of ExampleAlgorithm. Note that the population size of the PSO should match the number of instances; otherwise, unexpected errors may occur. Here, we need to transform the solution in the outer workflow, as the `HPOProblemWrapper` must accept a dictionary as input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "params:\n",
      " tensor([[-3.1316e-04, -4.2392e-01, -2.0663e+00,  2.8299e+00]], device='cuda:0') \n",
      "\n",
      "result:\n",
      " tensor([9.0117e-06], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "class solution_transform(torch.nn.Module):\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        return {\"self.algorithm.hp\": x}\n",
    "\n",
    "\n",
    "outer_algo = PSO(7, -3 * torch.ones(4), 3 * torch.ones(4))\n",
    "monitor = EvalMonitor(full_sol_history=False)\n",
    "outer_workflow = StdWorkflow()\n",
    "outer_workflow.setup(outer_algo, hpo_prob, monitor=monitor, solution_transform=solution_transform())\n",
    "outer_workflow.init_step()\n",
    "for _ in range(20):\n",
    "    outer_workflow.step()\n",
    "monitor = outer_workflow.get_submodule(\"monitor\")\n",
    "print('params:\\n', monitor.topk_solutions, '\\n')\n",
    "print('result:\\n', monitor.topk_fitness)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We find a really good hyperparameter setting for the problem within 1 sec."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
